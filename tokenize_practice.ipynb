{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb214e06",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda install -c conda-forge umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83195bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "#from umap import UMAP \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0787a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dded91d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6909bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a2f3e92",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize, sent_tokenize\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple_preprocess\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_words = [\n",
    "    \"thou\", \"thee\", \"thy\", \"thine\", \"ye\", \"you\", \"your\", \"yours\", \"shall\",\n",
    "    \"art\", \"hath\", \"hast\", \"wilt\", \"shalt\", \"shouldst\", \"couldst\", \"canst\",\n",
    "    \"dost\", \"doth\", \"didst\", \"were\", \"wast\", \"wert\", \"be\", \"been\", \"am\", \n",
    "    \"is\", \"are\", \"was\", \"unto\", \"of\", \"in\", \"for\", \"with\", \"on\", \"at\", \"by\",\n",
    "    \"through\", \"to\", \"from\", \"up\", \"down\", \"out\", \"as\", \"so\", \"but\", \"and\",\n",
    "    \"also\", \"therefore\", \"wherefore\", \"verily\", \"lo\", \"behold\", \"yea\", \"nay\",\n",
    "    \"moreover\", \"nevertheless\", \"albeit\", \"notwithstanding\", \"whereas\", \"whereunto\"\n",
    "]\n",
    "sw.extend(bible_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed23f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    x = x.lower()\n",
    "    words = word_tokenize(x)\n",
    "    words = [w for w in words if w not in sw]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21866c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../../../../Data/Text/conference_talks_2000s.txt', sep='\\t')\n",
    "url = 'https://github.com/esnt/Data/raw/main/Text/conference_talks_2000s.txt'\n",
    "df = pd.read_csv(url, sep='\\t')\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec11fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbbedf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['talk'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0:5,'talk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b17ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc34657",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(max_df=.7, min_df=200)\n",
    "X = tf.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9366ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xdf = pd.DataFrame(X.toarray(), columns=tf.get_feature_names_out())\n",
    "# Xdf.index = df['year']\n",
    "# Xdf.head(3)\n",
    "# year_max = Xdf.groupby(level=0).sum()\n",
    "# year_max.idxmax(axis=1)\n",
    "# for index, row in year_max.iterrows():\n",
    "#     print(index)\n",
    "#     print(row.nlargest(3))\n",
    "#     print('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a32132",
   "metadata": {},
   "outputs": [],
   "source": [
    "um = UMAP(2)\n",
    "vecs = um.fit_transform(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ab220",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame(vecs, columns=['v1','v2'])\n",
    "plot_df['text'] = tf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e0217",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(data_frame=plot_df, x='v1', y='v2', text='text')\n",
    "fig.update_traces(textposition='top center')\n",
    "fig.update_layout(width=1500, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b526d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c96d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## word embeddings.\n",
    "##  Data should be tokenized by sentence, then by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70af0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = []\n",
    "for doc in df['talk']:\n",
    "    sentences = sent_tokenize(doc)\n",
    "    for sentence in sentences:\n",
    "        words = simple_preprocess(sentence)\n",
    "        tokenized_sentences.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train model with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91db4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('atone', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors = model.wv.vectors\n",
    "# words = list(model.wv.index_to_key)\n",
    "tf_word_vectors = [model.wv[word] for word in tf.get_feature_names_out() if word in model.wv]\n",
    "# Reduce dimensions\n",
    "um2 = UMAP(n_neighbors=5, min_dist=0.3, n_components=2)\n",
    "umap_embeddings = um2.fit_transform(tf_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f013805",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"word\": tf.get_feature_names_out(),\n",
    "    \"dim1\": umap_embeddings[:, 0],\n",
    "    \"dim2\": umap_embeddings[:, 1]\n",
    "}\n",
    "we_df = pd.DataFrame(data)\n",
    "\n",
    "# Create the plot\n",
    "fig = px.scatter(we_df, x=\"dim1\", y=\"dim2\", text=\"word\")\n",
    "fig.update_traces(textposition='top center')\n",
    "fig.update_layout(width=1500, height=1000)\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['woman','king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaabda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81514617",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download from https://github.com/harmanpreet93/load-word2vec-google?tab=readme-ov-file\n",
    "## (file is 1.6GB)\n",
    "\n",
    "# from gensim.models import KeyedVectors\n",
    "# pretrained_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "##--------\n",
    "## OR\n",
    "##--------\n",
    "## code takes about 2 minutes to run\n",
    "\n",
    "# import gensim.downloader as api\n",
    "# pretrained_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff5d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.most_similar(positive=['woman','king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2499eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_wv_pretrain = [pretrained_model[word] for word in tf.get_feature_names_out() if word in pretrained_model]\n",
    "words = [word for word in tf.get_feature_names_out() if word in pretrained_model]\n",
    "# Reduce dimensions\n",
    "um2 = UMAP(n_neighbors=5, min_dist=0.3, n_components=2)\n",
    "umap_embeddings2 = um2.fit_transform(tf_wv_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b17ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"word\": words,\n",
    "    \"dim1\": umap_embeddings2[:, 0],\n",
    "    \"dim2\": umap_embeddings2[:, 1]\n",
    "}\n",
    "we_df2 = pd.DataFrame(data)\n",
    "\n",
    "# Create the plot\n",
    "fig = px.scatter(we_df2, x=\"dim1\", y=\"dim2\", text=\"word\")\n",
    "fig.update_traces(textposition='top center')\n",
    "fig.update_layout(width=1500, height=1000)\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4fd14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
